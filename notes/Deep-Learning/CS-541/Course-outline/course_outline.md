---
layout: post
title: Deep Learning Course Outline 
author: Bryan
hasmath: "true"
custom_css: tufte
---

# Course Outline

[TOC]



































---

# Homework:

## Homework 1:
* [Homework 1](notes/Deep-Learning/CS-541/Homeworks/hw1/Homework1.md)

## Homework 2:
* [Homework 2](notes/Deep-Learning/CS-541/Homeworks/hw2/Homework2.md)

## Homework 3:
* [Homework 3](notes/Deep-Learning/CS-541/Homeworks/hw3/Homework3.md)

## Homework 4:
* [Homework 4](notes/Deep-Learning/CS-541/Homeworks/hw4/Homework4.md)

## Homework 5:
* [Homework 5](notes/Deep-Learning/CS-541/Homeworks/hw5/Homework5.md)

## Homework 6:
* [Homework 6](notes/Deep-Learning/CS-541/Homeworks/hw6/Homework6.md)

## Homework 7:
* [Homework 7](notes/Deep-Learning/CS-541/Homeworks/hw7/Homework7.md)

---

# Part 1: Shallow Neural Networks & Optimization Methods

## **Mathematical Fundamentals:**

### Notation
* [Notation](notes/Deep-Learning/CS-541/notation/notation.md)

### Introduction
* [Notes 1]("notes/Deep-Learning/CS-541/Lect1/notes1.md")

### Linear Algebra

### Probability Theory

## **Shallow Architectures:**

### Linear Regression

* [Notes 2](notes/Deep-Learning/CS-541/Lect2/notes2.md)

### Logistic & Softmax Regression

## **Latent Variable Models:**

### Principal Component Analysis (PCA)

## **First-Order Optimization Methods:**

### Gradient Descent

* [Notes 2](notes/Deep-Learning/CS-541/Lect2/notes2.md)

### Stochastic Gradient Descent (SGD)

* [Notes 2](notes/Deep-Learning/CS-541/Lect2/notes2.md)

## **Second-Order Optimization Methods:**

### Newton's Method

## **Data pre-processing:**

### Whitening transformations

---

# Part 2: Deep Neural Networks & Optimization Methods

## **Feed-forward Architectures:**

### Multi-layer perceptrons (MLPs)

### Auto-encoders

### Convolutional neural networks (CNNs)

## **Recurrent architectures:**

### Recurrent neural networks (RNNs)

## **Generative architectures:**

### Variational auto-encoders (VAEs)

### Generative-adversarial networks (GANs)
## **Geometric Deep Learning:**

### Graph convolutional NNs

## **Optimization techniques:**

### Activation functions

### Batch/layer/group normalization

### Weight initialization

### Second-order approximation methods

## **Regularization techniques:**

### Weight decay

### Dropout

### Data augmentation

### Pre-training

### Weight sharing

---

# Part 3: State-of-the-art deep learning & applications

## **Recent Papers From:**

### NeurIPS

### ICML

### ICLR

### AAAI

### CVPR

### Arxiv

---