---
layout: notes_template
title: optimization
author: Bryan
tags: notes
custom_css: styles
hasmath: "false"
---



# Optimization

[TOC]

## Automatic Differentiation/Backpropagation

* [Youtube: What is Automatic Differentiation?](https://www.youtube.com/watch?v=wG_nF1awSSY)
* [Computing Gradients with Backpropagation](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/backprop.pdf)
* [Fast Exact Multiplication by the Hessian](http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf)
* [Automatic Differentiation Article](https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/packages/autograd/index.html)
* [Understanding Autograd: 5 Pytorch tensor functions](https://medium.com/@namanphy/understanding-autograd-5-pytorch-tensor-functions-8f47c27dc38)
* [Tutorial 1: Gradient Descent and AutoGrad](https://deeplearning.neuromatch.io/tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html)
* [Lecture 4: Backpropagation and Automatic Differentiation](http://dlsys.cs.washington.edu/pdf/lecture4.pdf)
* [Youtube: L3/2 Automatic Differentiation](https://www.youtube.com/watch?v=RP0JScZG6gA&list=WL&index=68)
* [Computing Higher Order Derivatives of Matrix and Tensor Expressions](http://www.matrixcalculus.org/matrixcalculus.pdf)
* [The Stan Math Library: Reverse-Mode Automatic Differentiation in C++](https://arxiv.org/pdf/1509.07164.pdf)
* [automatic differentiation presentation](https://www.cs.usask.ca/~spiteri/CMPT898/notes/ad.pdf)
* [Automatic Differentiation for Matrices (Differentiable Programming)](https://www.deep-teaching.org/notebooks/differentiable-programming/exercise-automatic-differentiation-matrix)
* [PyTorch Autograd Explained - In-depth Tutorial](https://www.youtube.com/watch?v=MswxJw-8PvE&list=LL&index=6) **really good for initial understanding**
* [Automatic Differentiation](https://www.youtube.com/watch?v=R_m4kanPy6Q&list=LL&index=4)
* [Deep Learning Framework From Scratch Using Numpy](https://arxiv.org/pdf/2011.08461.pdf)
* [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)
* [Deep Learning Programming Paradigm](https://mxnet.apache.org/versions/1.8.0/api/architecture/program_model.html) **pretty good explanations on automatic differentiation**
* [Automatic Differentiation for Scalars (Differentiable Programming)](https://gitlab.com/deep.TEACHING/educational-materials/blob/master/notebooks/differentiable-programming/exercise-automatic-differentiation-scalar.ipynb)
* [Reverse-mode automatic differentiation from scratch, in Python](https://sidsite.com/posts/autodiff/)
* [Introduction to vector calculus and partial derivatives](https://explained.ai/matrix-calculus/#sec3)
* [Livecoding an Autograd Library](https://www.youtube.com/playlist?list=PLeDtc0GP5ICldMkRg-DkhpFX1rRBNHTCs)



## Nesterov Acceleration 

* [Great Article On Nesterov Accelerated Gradient and Momentum](https://jlmelville.github.io/mize/nesterov.html)
* [Papers With Code: Nesterov Accelerated Gradient](https://paperswithcode.com/method/nesterov-accelerated-gradient)
* [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)
* [Nesterovâ€™s Optimal Gradient Methods Presentation](https://www2.cs.uic.edu/~zhangx/teaching/agm.pdf)
* [Gradient Descent With Nesterov Momentum From Scratch](https://machinelearningmastery.com/gradient-descent-with-nesterov-momentum-from-scratch/)
* [Nesterov Accelerated Gradient Youtube Video](https://www.youtube.com/watch?v=uHOTRHqnakQ)



## Optimization For Deep Learning (Course)

### Other assignments that helped me see problems from a different angle

* [Convergence Theorems for Gradient Descent](https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf)
* [Optimization Lecture by, Ryan Tibshirani](https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf)

