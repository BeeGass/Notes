I"Ñ<h1 id="lecture-1-intro-and-general-overview-of-linear-regression">Lecture 1: Intro and general overview of linear regression</h1>

<p>Feed-forward networks consisting of multiple layers of neurons, each of which feeds to the next layer</p>

<p><img src="notes\Deep Learning Notes\neural network-1.png" alt="2-layer Neural Network" /></p>

<p>2-layer Neural Network</p>

<p>Let dataset $D =  {(x^{(i)}, y^{(i)})}^{n}_{i=1}$</p>

<p><strong>x</strong> relates to the to the matrix that contains $x_1$, ‚Ä¶, $x_m$</p>

<p>The output layer $\hat{y}$ computes the sum of the inputs multiplied by weights</p>

\[\hat{y} = g(x; w) = \sum^{m}_{i=1} x_{i} w_{i} = x^{T} w\]

<h2 id="what-is-deep">What is ‚Äúdeep‚Äù?</h2>

<p>In ‚Äúclassical‚Äù Machine Learning (e.g., SVMs, boosting, decision trees), f
is often a ‚Äúshallow‚Äù function of <strong>x</strong>, e.g.:
\(\hat{y} = f(x) = x^{T} w\)
In contrast, with DL, f is the <strong>composition</strong> (possibly 1000s
of ‚Äúlayers‚Äù deep!) of many functions, e.g.:
\(f(x) = f_n ( ... (f_2 (f_1 (x))))\)
define the machine by a function <em>g</em> (with parameters w) whose output $\hat{y}$ is linear in its inputs:</p>

<p>\(\hat{y} = g(x; w) = \sum^{m}_{i=1} x_{i} w_{i} = x^{T} w\)
this is equivalent to a 2-layer neural network (with no activation function):</p>

<ul>
  <li>$x_{i}$ refers to the input layer that can range from $x_{i}, ‚Ä¶, x_{m}$</li>
  <li>$w_i$ refers to the weights that $x_i$ is multipled by</li>
  <li>The combination of $x_i$ and $w_i$ creates the output of $\hat{y}$, the prediction
    <ul>
      <li>This prediction is compared to the target value in some way</li>
    </ul>
  </li>
</ul>

:ET