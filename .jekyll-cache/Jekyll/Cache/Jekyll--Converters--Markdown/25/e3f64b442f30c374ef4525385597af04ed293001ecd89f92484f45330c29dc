I"DÅ<h1 id="lecture-3">Lecture 3:</h1>

<p>[TOC]</p>

<h2 id="quick-note">Quick Note:</h2>

<p>If you want a more detailed overview of material after this lecture please read chapters 1, 2 and 5 in the Deep Learning Book</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>Problem:</p>

<p>For the 2-layer neural network below, let m=2 (the number of input neurons or features) and $w^{(0)} = [1 \; 0]^{T}$.</p>

<p>Compute the updated weight vector $w^{(1)}$ after one iteration of gradient descent using MSE loss, a single training example $(x, y) = ([2, 3]^{T}, 4)$, and learning rate $\epsilon = 0.1$.</p>

<p>Attempt:
$$
\begin{align*}
	w^{(0)} &amp;= [1 \; 0]^{T} \ \</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\textbf{w}^{(1)} &amp;= (\textbf{X} \textbf{X}^{T})^{-1}(\textbf{X}y) \\ \\

\textbf{w}^{(2)} &amp;= \textbf{w}^{(0)} - \epsilon * \nabla_{w^{(1)}}f_{MSE}(\textbf{w}) \\ \\

\textbf{w}^{(2)} &amp;= \textbf{w}^{(0)} - \epsilon * \frac{1}{n}(\textbf{X}(\textbf{X}^{T}\textbf{w} - y)) \\ \\

\textbf{w}^{(2)} &amp;= [1 \; 0]^{T} - 0.1 * ([2, 3]([2, 3]^{T} \cdot [0 \; 1] - [4])) \\ \\

\textbf{w}^{(2)} &amp;= \begin{bmatrix} 
														1 + 0.1 * 2 * 2 \\
                                                        0 + 0.1 * 3 * 2 \\
												 	  \end{bmatrix} \\ \\

\textbf{w}^{(2)} &amp;= \begin{bmatrix} 
														1.4 \\
                                                        0.6 \\
												 	  \end{bmatrix} \\ \\
</code></pre></div></div>

<p>\end{align*}
$$</p>

<h2 id="probabilistic-machine-learning">Probabilistic Machine Learning</h2>

<p>Sometimes we may be very uncertain about our prediction of the target value y from the input x.</p>

<p><img src="C:\Users\Bryan\Pictures\DL Notes Pics\lec3-Prob1.png" alt="" /></p>

<p>Here you can see that the point‚Äôs y value contains a lot of uncertainty due in part from the multiple points that lie at similar x values. This regression does not accurately represent the data given.</p>

<p>Instead we should use a predicative distribution for our regression as shown here</p>

<p><img src="C:\Users\Bryan\Pictures\DL Notes Pics\lec3-Prob2.png" alt="" /></p>

<p>it turns out that the optimal parameters for a conditional Gaussian probability model are exactly the same as for linear regression with minimal MSE</p>

<h2 id="probabilistic-deep-learning-intro">Probabilistic Deep Learning Intro</h2>

<p>Neural Networks can be used in various ways to make probabilistic predictions:</p>

<ul>
  <li>For regression, estimate both the expected value and the variance of the prediction</li>
  <li>Model a high-dimensional distribution using a probabilistic latent variable model (LVM) - akin to factor analysis but deeper</li>
</ul>

<h2 id="definitions">Definitions:</h2>

<h3 id="random-variables">Random Variables</h3>

<p>A random variable (denoted sometimes as <strong>RV</strong> ) usually takes the form like $X$ (with sample space $\Omega$) has a value we are unsure about, maybe because</p>

<ul>
  <li>it is decided by some random process</li>
  <li>
    <p>it is hidden from us</p>
  </li>
  <li>RV‚Äôs are typically written as capital letters, e.g. $X$, $Y$</li>
  <li>Once the value of the RV, $X$, has been ‚Äúsampled‚Äù, ‚Äú‚Äúselected‚Äù, ‚Äúinstantiated‚Äù, or ‚Äúrealized‚Äù (by a random number generator, generative process, God, etc.), it takes a specific value from the sample space</li>
  <li>The values the RV can take are typically written as lowercase letters, e.g., $x$, $y$.</li>
</ul>

<p>Types of sample spaces $\Omega$:</p>

<ul>
  <li>Finite, e.g.:
    <ul>
      <li>${ 0, 1 }$</li>
      <li>${ \text{red, blue, green} }$</li>
    </ul>
  </li>
  <li>Countable, e.g.:
    <ul>
      <li>$\Bbb{Z}_{\geq 0}$</li>
    </ul>
  </li>
  <li>Uncountable, e.g.:
    <ul>
      <li>$\Bbb{R}$</li>
    </ul>
  </li>
</ul>

<p>The probability that a random variable $X$ takes a particular value is determined by a:</p>

<ul>
  <li>Probability mass function (PMF) for finite or countable sample spaces.</li>
  <li>Probability density function (PDF) for uncountable sample spaces.</li>
</ul>

<h2 id="pmf">PMF</h2>

<p>Example 1 (finite):</p>

<ul>
  <li>Let RV $X$ be the outcome of rolling a 6-sided die.</li>
  <li>If $X$ is fair, then:</li>
</ul>

\[P(X = i) = \frac{1}{6} \; \forall i \in \{1, ..., 6 \}\]

<p>Example 2 (countable):</p>

<ul>
  <li>Let RV $X$ be the number of TCP/IP packets that arrive in 1 second.</li>
  <li>We can model the count of packets with a Poisson distribution:</li>
</ul>

\[P(X = k) = \frac{\lambda^{k} e^{-\lambda}}{k!}\]

<p>where parameter $\lambda$ specifies the rate of the packet arrivals</p>

<p><img src="C:\Users\Bryan\Pictures\DL Notes Pics\lec3-Prob3.png" alt="" /></p>

<ul>
  <li>
    <p>Example 1:</p>

    <ul>
      <li>let $X$ be a uniformly-distributed RV over $\Omega = [0, 1]$.</li>
      <li>Then $f_{X}(x) = 1 \; \forall x \in \Omega$</li>
    </ul>

    <p><img src="C:\Users\Bryan\Pictures\DL Notes Pics\lec3-Prob4.png" alt="" /></p>
  </li>
  <li>
    <p>Example 2:</p>

    <ul>
      <li>Let Y be a uniformly-distributed RV over $\Omega = [\frac{1}{4},  \frac{3}{4}]$</li>
      <li>Then $f_{Y}(y) = 2 \; \forall y \in \Omega$</li>
    </ul>

    <p><img src="C:\Users\Bryan\Pictures\DL Notes Pics\lec3-Prob5.png" alt="" /></p>

    <p><strong>Note that the PDF can exceed 1</strong> due in part because the integral must be equal to 1 in all cases</p>
  </li>
  <li>
    <p>Example 3:</p>

    <ul>
      <li>Let $Z$ be a <strong>normally</strong> (aka Gaussian) distributed RV with mean 1.5 (location parameter) and variance 4 (width parameter), i.e.,</li>
    </ul>

\[Z \sim \mathcal{N}(z; \mu = 1.5, \sigma^{2} = 4)\]

    <ul>
      <li>Then</li>
    </ul>

\[f_{Z}(z) = \frac{1}{\sqrt{2\pi \sigma^2}} \; \text{exp}(-\frac{(z - \mu)^{2}}{2\sigma^{2}})\]

    <p><img src="C:\Users\Bryan\Pictures\DL Notes Pics\lec3-Prob6.png" alt="" /></p>
  </li>
</ul>

<h3 id="side-note">Side Note:</h3>

<ul>
  <li>In this course, we will relax notation and use ‚Äúprobability distribution‚Äù to mean either the PDF or PMF of a RV (as appropriate)</li>
  <li>As a notational shortcut, we use $P(x)$ to mean $P(X=x)$ or $f_{X}(X)$</li>
</ul>

<h2 id="joint-probability-distributions">Joint Probability Distributions</h2>

<ul>
  <li>For multiple random variables $X, Y, ‚Ä¶,$ we can construct the joint probability distribution $P(x, y, ‚Ä¶)$ to mean the probability that $(X=x) \and (Y=y) \; \and \; ‚Ä¶.$
    <ul>
      <li>$P(x, y, ‚Ä¶)$ must sum to one</li>
    </ul>
  </li>
  <li>Note that $P$ must still sum to 1 over all possible joint values $(x, y, ‚Ä¶)$.</li>
</ul>

<p>Example in 2-D ‚Äì crayons:</p>

<ul>
  <li>Let $X$ be the color (red, blue, green, white)</li>
  <li>Let $Y$ be the intensity (low, medium, high)</li>
</ul>

\[\begin{align*}
	\begin{matrix}
		\text{} &amp; \text{Red} &amp; \text{Blue} &amp; \text{Green} &amp; \text{White} \\
        \text{Low} &amp; 0.1 &amp; 0.05 &amp; 0.025 &amp; 0.2 \\
        \text{Med} &amp; 0.075 &amp; 0.05 &amp; 0.1 &amp; 0 \\
        \text{High} &amp; 0.25 &amp; 0.05 &amp; 0.075 &amp; 0.025 \\
	\end{matrix}
\end{align*}\]

<p>Q: What is the overall probability of picking a white canyon at random?</p>

<p>A: 0.225</p>

<p>From the joint distribution we can compute the marginal distributions $P(x)$ and $P(y)$.
\(\begin{align}
    P(x) &amp;= \sum_{y} P(x, y) \\ \\
    P(y) &amp;= \sum_{x} P(x, y)
\end{align}\)</p>

<p>\(\begin{align*}
	\begin{matrix}
		\text{} &amp; \text{Red} &amp; \text{Blue} &amp; \text{Green} &amp; \text{White} &amp; P(y) \\
        \text{Low} &amp; 0.1 &amp; 0.05 &amp; 0.025 &amp; 0.2 &amp; 0.375\\
        \text{Med} &amp; 0.075 &amp; 0.05 &amp; 0.1 &amp; 0 &amp; 0.225\\
        \text{High} &amp; 0.25 &amp; 0.05 &amp; 0.075 &amp; 0.025 &amp; 0.4\\
        \text{P(x)} &amp; 0.425 &amp; 0.15 &amp; 0.2 &amp; 0.225
	\end{matrix}
\end{align*}\)
This is also called the law of total probability:</p>

<h3 id="law-of-total-probability">Law Of Total Probability</h3>

<ul>
  <li>
    <p>For any RVs $X$ and $Y$:
\(P(x) = \sum_{y} P(x, y)\)</p>

    <h3 id="joint-probability-distributions-continued">Joint Probability Distributions Continued</h3>

    <p>In machine learning, we often use joint distributions of many variables that are part of a collection, e.g.:</p>
  </li>
  <li>
    <p>Sequence $(W_{1}, W_{2}, ‚Ä¶, W_{T})$ of words in a sentence</p>

    <ul>
      <li>$W_{t}$ is the $t^{th}$ RV in the sequence (representing a word that ‚Äúyou‚Äù picked at random from a bag of words)</li>
    </ul>
  </li>
  <li>
    <p>Grid $(I_{11}, ‚Ä¶, I_{1M}, ‚Ä¶, I_{N1}, ‚Ä¶, I_{NM})$ of the pixels in an N x M image.</p>
  </li>
</ul>

<h2 id="conditional-probability-distributions">Conditional Probability Distributions</h2>

<p>Sometimes the value of one RV is predictive of the value of another RV</p>

<p>Examples:</p>

<ul>
  <li>If I know a person‚Äôs height $H$, then I have some information about their weight $W$.</li>
  <li>If I know how much cholesterol $C$ a person eats, then I have some information about their chance of coronary heart disease $D$</li>
</ul>

<p>We can form a conditional probability distribution of RV $X$ given the value of RV $Y$:
\(P(x \;| \;y)\)
the bar in between $x$ and $y$ meaning ‚Äúconditional on‚Äù or ‚Äúgiven‚Äù</p>

<p>Examples:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Height given weight: $P(h \;</td>
          <td>\; w)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Heart disease given cholesterol: $P(d \;</td>
          <td>\; c)$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>More generally, we can form a conditional probability distribution of $X_{1}, ‚Ä¶, X_{n}$ given the values of  $Y_{1}, ‚Ä¶, Y_{m}$: 
\(P(x_{1}, ..., x_{n} \;|\; y_{1}, ..., y_{m})\)
A conditional probability distribution is related to the joint probability distribution as follows:
\(P(x \; | \; y) P(y) = P(x, y)\)
It follows that:
\(P(x \; |\; y, z)P(x \; |\; y) = P(x, y \; | \; z)\)
More generally:
\(P(x_{1}, ..., x_{n} \; |\; y_{1}, ..., y_{m})P(y_{1}, ..., y_{m}) = P(x_{1}, ..., x_{n}, y_{1}, ..., y_{m})\)
And also:
\(\begin{align*}
    P(x_{1}, ..., x_{n} \; |\; y_{1}, ..., y_{m}, z_{1}, ..., z_{p}) P(y_{1}, ..., y_{m} \; &amp;|\; z_{1}, ..., z_{p})&amp; \\ = P(x_{1}, ..., x_{n}, y_{1}, ..., y_{m}) \; &amp;| \; z_{1}, ..., z_{p})
\end{align*}\)
Note that the same joint probability can be factored in different ways, e.g.:
\(\begin{align*}
	P(x,y,z) &amp;= P(x, y \; | \; z)P(z) \\ \\
	&amp;= P(x\; | \; y, z)P(y, z)
\end{align*}\)</p>

<h3 id="exercises">Exercises:</h3>

<ol>
  <li>
    <p>$P(a, b, c, d) = P(a, c) \; * \; ?$</p>

    <p>A: 
\(P(a,c) \; * \; P(b,d \; | \; a, c)\)</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(W_{1}, W_{2}, W_{3}) = P(W_{3}\;</td>
          <td>\; W_{1}) \; * \; ? \; * \; ?$</td>
        </tr>
      </tbody>
    </table>

    <p>A: 
\(P(W_{3}\; |\; W_{1}) \; * \; P(W_{1}) \; * \; P(W_{2} \; |\; W_{1}, W_{3})\)</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(X_{1}, X_{2}, X_{3}) = P(X_{1}) \; * \; ? \; * \; P(X_{3}\;</td>
          <td>\; X_{1}, X_{2})$</td>
        </tr>
      </tbody>
    </table>

    <p>A:
\(P(X_{1}, X_{2}, X_{3}) = P(X_{1}) \; * \; P(X_{2} \; | \; X{1}) \; * \; P(X_{3}\; | \; X_{1}, X_{2})\)</p>
  </li>
  <li>
    <p>$P(X_{1}, ‚Ä¶, X_{n}) = P(X_{1}) \; * \; \underset{\text{n-1 terms}}{? \; * \; ? \; * \; ‚Ä¶ \; * \; ?}$</p>

    <p>A:
\(P(X_{1}, ..., X_{n}) = P(X_{1}) \; * \; \underset{i=2}{\overset{n}{\Pi}}P(X_{i} \; | \; X_{1}, ..., X_{i-1})\)</p>
  </li>
</ol>

<h2 id="independence">Independence</h2>

<p>RVs $X$ and $Y$ are independent i.f.f. $P(x,y) = P(x)P(y) \; \forall x, y,$ i.e., the joint distribution equals the product of the marginal distributions.</p>

<table>
  <tbody>
    <tr>
      <td>Note that this implies that $P(x \;</td>
      <td>\;  y) = P(x)$ and $P(y \;</td>
      <td>: x) = P(y)$ since $P(x,y) = P(x \;</td>
      <td>\; y )P(y) = P(y \;</td>
      <td>\; x)P(x)$ by definition of conditional probability.</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>in simpler terms the above means that probability of $x$ given $y$ is simply the probability of $x$ if $y$ completely independent of $x$ and gives no further information toward the probability of $x$. The same is true for probability of $y$.</li>
</ul>

<h2 id="conditional-independence">Conditional independence</h2>

<p>RVs $X$ and $Y$ are conditionally independent given RV Z iff:
\(P(x,y \; |\; z) = P(x \; | \; z)P(y \; | \; z) \; \forall x, y,z\)
Note that this implies:
\(P(x \; | \; y, z) = P(x \; | \; z)\)
In words: ‚ÄúIf I already know the value of $Z$, then knowing $Y$ tells me nothing further about $X$‚Äù</p>

<h3 id="generalized-form">Generalized Form:</h3>

<p>More generally: $X_{1}, ‚Ä¶, X_{n}$ and $Y_{1}, ‚Ä¶, Y_{m}$ are conditionally independent given $Z_{1}, ‚Ä¶, Z_{p} iff:$
\(\begin{align*}
    P(x_{1}, ..., x_{n}, y_{1}, ..., y_{m} \; &amp;| \; z_{1}, ..., z_{p}) \\ = P(x_{1}, ..., x_{n} \; |\; z_{1}, ..., z_{p})P(y_{1}, ..., y_{m} \; &amp;| \; z_{1}, ..., z_{p})
\end{align*}\)</p>

<h2 id="bayes-rule">Bayes‚Äô rule</h2>

<table>
  <tbody>
    <tr>
      <td>It is often useful to compute $P(x \;</td>
      <td>\; y)$ in terms of $P(y \;</td>
      <td>\; x)$.</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>example
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>if $X$ represents a student‚Äôs skill level, and $Y$ is their test score, it‚Äôs often easier to compute $P(y \;</td>
              <td>\; x)$. But given a student‚Äôs test score $Y$, we really want to know $P(x \;</td>
              <td>\; y)$.</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<p>Bayes‚Äô rule:
\(P(x \; | \; y) = \frac{P(x, y)}{P(y)} = \frac{P(y \; | \; x)P(x)}{P(y)}\)
We can also generalize Bayes‚Äô rule to cases where we always condition on a tertiary variable $Z$:
\(P(x \; | \; y, z) =  \frac{P(y \; | \; x, z)P(x \; | \; z)}{P(y \; | \; z)}\)
It is sometimes possible ‚Äî and more convenient ‚Äî to work with <strong>unnormalized</strong> probabilities</p>

<p>For instance, it might suffice to know that</p>

<table>
  <tbody>
    <tr>
      <td>$[P(y^{(1)} \;</td>
      <td>\; x), \; P(y^{(2)} \;</td>
      <td>\; x), \; P(y^{(3)} \;</td>
      <td>\; x)] \; \propto [3.5, 7, 0.04]$</td>
    </tr>
  </tbody>
</table>

<p>rather than their exact (normalized) values.</p>

<h2 id="probabilistic-inference">Probabilistic Inference</h2>

<p>To express the conditional independence relationships between multiple RVs, it is useful to represent their dependencies in a graph.</p>

<p>A formal theory of probabilistic graphical models (Pearl 1998) has been devised.</p>

<ul>
  <li>Conditional independence can be determined via the principle of <strong>d-separation</strong> (beyond the scope of this course).</li>
</ul>

<h3 id="probabilistic-graphical-models">Probabilistic graphical models</h3>

<p>Example 1</p>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\Probabilistic Graph.png" alt="" /></p>

<ul>
  <li>C: whether the patients eats caviar.</li>
  <li>S: the patient‚Äôs sex</li>
  <li>H: whether the patient has high cholesterol</li>
  <li>A: whether the patient will have a heart attack.</li>
  <li>
    <p>B: whether the patient has shortness of breath.</p>
  </li>
  <li>This model implies that:</li>
</ul>

\[\begin{align*}
	P(a, b \; | \; h,c,s) &amp;= P(a, b \; | \; h) \text{ and} \\ 
	P(c, s \; | \; h, a, b) &amp;= P(c, s \; | \; h)
\end{align*}\]

<ul>
  <li>In words, ‚ÄúIf I want to know the probability the patient will have a heart attack A, and I already know the patient has high cholesterol H, then the patient‚Äôs sex and whether she/he eats caviar C is irrelevant.‚Äù</li>
</ul>

<p>Example 2: Markov Chain</p>

<p>This graph shows elements of a time series</p>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\Probabilistic Graph (1).png" alt="" /></p>

<ul>
  <li>This chain-like model of $X_{1}, ‚Ä¶, X_{n}$ implies that:</li>
</ul>

\[\begin{align*}
	P(x_{i} \; | \; x_{1}, ..., x_{i-1}) &amp;= P(x_{i} \; | \; x_{i-1}) \text{ and} \\
	P(x_{i} \; | \; x_{i+1}, ..., x_{n}) &amp;= P(x_{i} \; | \; x_{i+1})
\end{align*}\]

<p>In words, ‚ÄúIf I want to know the value of $X_{i}$ and I already know $X_{i-1}$, then the values of any ‚Äòearlier‚Äô $X‚Äôs$ are irrelevant.‚Äù</p>

<p>Example 3</p>

<ul>
  <li>Given a model with multiple RVs and how they are related to each other, we can infer the values of other RVs.</li>
  <li>For the medical diagnosis example, suppose we knew the conditional probability distributions:</li>
</ul>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\Probabilistic Graph.png" alt="" /></p>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\table.png" alt="" />
\(P(H=h \; | \; C=c, S=s)\)
<img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\table1.png" alt="" /></p>

<p>Example 4</p>

<ul>
  <li>Suppose we meet a male patient who eats caviar.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>What is the <strong>posterior probability</strong> that H=1, i.e., $P(H=1 \;</td>
          <td>\; C=1, S=Ma)$? (Posterior means after observing C, S.) (given the diagram and tables from example 3)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>A: $0.6$</p>

<ul>
  <li>What if we also know that the patient is short of breath?</li>
</ul>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\table2.png" alt="" /></p>

<ul>
  <li>conditional independence from the graphical model</li>
</ul>

\[P(b \; | \; h, c, s) = P(b \; | \; h)\]

<p>$$
\begin{align*}
&amp;P(H=1 \; | \; C=1, S=Ma, B=1) \ <br />
= \; &amp;\frac{P(B=1 \; | \; H=1, C=1, S= Ma)P(H=1 \; | \; C=1, S= Ma)}{P(B=1 \; | \; C=1, S= Ma)} \text{ Bayes‚Äô Rule} \ <br />
= \; &amp;\frac{P(B=1 \; | \; H=1)P(H=1 \; | \; C=1, S= Ma)}{P(B=1 \; | \; C=1, S= Ma)} \text{ Conditional independence} \ <br />
= \; &amp;\frac{0.9 * 0.6}{\sum^{1}<em>{h=0} P(B=1, H=h \; | \; C=1, S= Ma)} \text{ Law of total probability} \ <br />
= \; &amp;\frac{0.54}{\sum^{1}</em>{h=0} P(B=1 \; | \; H=h, C=1, S= Ma)P(H=h \; | \; C=1, S= Ma)} \text{ Def. of cond. prob.} \ <br />
= \; &amp;\frac{0.54}{P(B=1 \; | \; H=h)P(H=h \; | \; C=1, S= Ma)} \text{ Conditional independence} \ \ 
= \; &amp;\frac{0.54}{0.1 * 0.4 * 0.9 * 0.6} \ <br />
= \; &amp;\frac{0.54}{0.04 * 0.54} \ <br />
\approx \; &amp;0.93</p>

<p>\end{align<em>}
\(Alternatively, it is often easier to work with unnormalized probabilities, i.e., values proportional to the probabilities\)
\begin{align</em>}
	&amp; \;P(H=1 \; | \; C=1, S=Ma, B=1) \ <br />
	&amp;\propto P(B=1 \;| \;H=1)P(H=1 \; | \; C=1, S=Ma) \ <br />
	&amp;= 0.9 * 0.6 \ \ \ 
	&amp;= P(H=0 \; | \; C=1, S=Ma, B=1) \ <br />
	&amp;\propto P(B=1 \; | \; H= 0)P(H=0 \; | \; C=1, S=Ma) \ <br />
	&amp;= 0.1 * 0.4 \ \</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&amp;P(H=1 \; | \; C=1, S=Ma, B=1) = \frac{0.9 * 0.6}{0.9 * 0.6 * 0.1 * 0.4} = 0.93 \\ \\
&amp;P(H=0 \; | \; C=1, S=Ma, B=1) = \frac{0.1 * 0.4}{0.9 * 0.6 + 0.1 * 0.4} = 0.07 \\ \\ 
&amp;\text{since the probabilities must sum to 1} \end{align*} $$
</code></pre></div></div>

<h2 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h2>

<h3 id="parameters-in-probability-distributions">Parameters In Probability Distributions:</h3>

<p>Most probabilistic models have parameters we want to estimate.</p>

<p>For example, the conditional probabilities for medical diagnosis are all parameters that must be learned.</p>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\table3.png" alt="" /></p>

<p>Most probabilistic models have parameters we want to estimate.</p>

<p>As another example, we might want to estimate the bias B of a coin after observing n coin flips $H_{1}, ‚Ä¶, H_{n}$:</p>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\Probabilistic Graph (2).png" alt="" />
\(\begin{align*}
	P(H_{i} = 1 \; &amp;| \; b) = b \\
    \text{Conditional} &amp;\text{ independence:} \\
    P(h_{i} \; | \; b, h_{1}, ..., h_{i-1}&amp;, h_{i+1}, ...,h_{n}) =P(h_{i} \; | \; b)
\end{align*}\)
What is a principled approach to estimating B?</p>

<p>Maximum likelihood estimation (MLE):</p>

<ul>
  <li>The value of a latent variable is estimated as the one that makes the observed data as likely (probable) as possible.</li>
</ul>

<p>the likelihood of $H_{1}, ‚Ä¶, H_{n}$ given B is:
\(\begin{align*}
	&amp;=P(h_{1}, ..., h_{n} \; | \; b) = P(h_{1} \; | \; b) \; \overset{n}{\underset{i=2}{\Pi}} P(h_{i} \; | \; b, h_{1}, ..., h_{i-1}) \\
	&amp;=P(h_{1} \; | \; b) \overset{n}{\underset{i=2}{\Pi}} P(h_{i} \; | \; b) \; \text{ Conditional independence} \\
	&amp;= \overset{n}{\underset{i=1}{\Pi}} P(h_{i} \; | \; b)
\end{align*}\)
We can express the probability of each $h_{i}$ given $b$ as:
\(\begin{align*}
	P(h_{i} \; | \; b) &amp;= b^{h_{i}}(1-b)^{1-h_{i}} \\
	&amp;= b \text{ if } h_{i} = 1 \text{ or} \\
	&amp;\; \; \; \; (1-b) \text{ if } h_{i} = 0 \\
\end{align*}\)
The exponent ‚Äúchooses‚Äù the correct probability for $H_{i} = 1$ or $H_{i} = 0$.</p>

<p>We seek to maximize the probability of $h_{1}, ‚Ä¶, h_{n}$ by optimizing $b$.</p>

<p>It‚Äôs often easier instead to optimize the log-likelihood.
\(\begin{align*}
	&amp;\text{arg } \underset{b}{\text{max}}P(h_{1}, ..., h_{n} \; | \; b) = \text{arg } \underset{b}{\text{max}} \text{ log}P(h_{1}, ..., h_{n} \; | \; b)^{*} \\
&amp;\text{assuming the probability is never exactly 0}
\end{align*}\)</p>

\[\begin{align*}
	\text{log } P(h_{1}, ..., h_{n} \; | \; b) &amp;= \text{log } \overset{n}{\underset{i=1}{\Pi}} P(h_{i} \; | \; b) \; \text{ due to conditional independence} \\ 
	&amp;= \sum^{n}_{i=1} \text{ log}P(h_{i} \; | \; b) \\
	&amp;= \sum^{n}_{i=1} \text{ log }b^{h_{i}}(1-b)^{1-h_{i}} \\
	&amp;= \sum^{n}_{i=1} h_{i} \text{ log }b \; + \;(1-h_{i}) \text{ log }(1-b) \\
	n_{1} \text{ is number of heads. } \;\;\;\; \;\;\;\; &amp;= n_{1} \text{log }b \; + \;(n-n_{1})\text{ log }(1-b)
\end{align*}\]

<p>We can now differentiate w.r.t. b, set to 0, and solve to obtain the MLE of B</p>

<p>:
$$
\begin{align*}
	\nabla_{b} [n_{1}\text{ log } b \; + \; (n -n_{1}) \; \text{log }(1-b)] &amp;= \frac{n_{1}}{b} - \frac{(n - n_{1})}{1 - b} <br />
	(1-b) n_{1} - b(n - n_{1}) &amp;= 0 <br />
	n_{1} - bn_{1} - bn \; + \;bn_{1} &amp;= 0 <br />
    n_{1} &amp;= bn <br />
    b &amp;= \frac{n_{1}}{n} <br />
    &amp;\text{The MLE for B is the fraction of coin flips that are heads.}</p>

<p>\end{align*}
$$</p>

<h2 id="linear-gaussian-models">Linear-Gaussian Models</h2>

<p>Let‚Äôs consider a different model that contains real-valued RVs (not just from a finite sample space).</p>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\Linear-Gaussian Model.png" alt="" /></p>

<ul>
  <li>$X$ is some feature vector (e.g., face image).</li>
  <li>$Y$ is some outcome variable (e.g., age).</li>
  <li>$W$ is a vector of weights that characterize how $Y$ is related to $X$.</li>
  <li>$\sigma$  expresses how uncertain we are about $Y$ after seeing $X$.</li>
</ul>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\Linear-Gaussian Model 1.png" alt="" /></p>

<p>Suppose we model the relationship between X, W, œÉ, and Y such that:</p>

<ul>
  <li>$Y$ is a normal/Gaussian random variable.</li>
  <li>The expected value of $Y$ is $x^{T}w$.</li>
  <li>The variance of $Y$ is constant $(\sigma^{2})$ for all possible $x$.</li>
</ul>

<p>If we collect a dataset $D = { (x^{(i)}, y^{(i)})}^{n}_{i=1}$, what is the MLE for $W$ and $\sigma$?
\(P(y \; | \; w, x) = \mathcal{N}(y; x^{T}w, \sigma^{2})\)</p>

\[\begin{align}
	P(y \; | \; x, w, \sigma^{2}) &amp;= \mathcal{N}(y; x^{T}w, \sigma^{2}) = \frac{1}{\sqrt{2\pi \sigma^{2}}} \text{ exp}(-\frac{(y - x^{T}w)^2}{2\sigma^{2}}) \\
	P(D \; | \; w, \sigma^{2}) &amp;= \overset{n}{\underset{i=1}{\Pi}}P(y^{(i)} \; | \; x^{(i)}, w, \sigma^{2}) \text{ Conditional independence} \\
	\text{log }P(D \; | \; w, \sigma^{2}) &amp;= \text{log }\overset{n}{\underset{i=1}{\Pi}}P(y^{(i)} \; | \; x^{(i)}, w, \sigma^{2}) \\
	&amp;= \overset{n}{\underset{i=1}{\Sigma}} \text{log }P(y^{(i)} \; | \; x^{(i)}, w, \sigma^{2}) \\
	&amp;\text{for remaining portion of the proof refer to homework 2}\\
\end{align}\]

<ul>
  <li>MLE for <strong>w</strong>:</li>
</ul>

\[w = (\overset{n}{\underset{i=1}{\Sigma}} x^{(i)}x^{(i)^{T}})^{-1}(\overset{n}{\underset{i=1}{\Sigma}} x^{(i)}y^{(i)})\]

<p>This is the same solution as for linear regression, but derived as the MLE of a probabilistic model (instead of the minimum MSE).</p>

<ul>
  <li>MLE for $\sigma^{2}$:</li>
</ul>

\[\sigma^{2} = \frac{1}{n} \; \overset{n}{\underset{i=1}{\Sigma}} ((x^{(i)^{T}} w) - y^{(i)})^{2}\]

<p>This is the sum of squared residuals of the predictions w.r.t. ground-truth.</p>

<h2 id="l_2-regularization">$L_{2}$ Regularization</h2>

<h3 id="regularization">Regularization</h3>

<p>The larger the coefficients (weights) $w$ are allowed to be, the more the neural network can overfit.</p>

<p>If we ‚Äúencourage‚Äù the weights to be small, we can reduce overfitting</p>

<p>This is a form of regularization ‚Äî any practice designed to improve the machine‚Äôs ability to generalize to new data.</p>

<p>One of the simplest and oldest regularization techniques is to penalize large weights in the cost function.</p>

<ul>
  <li>The ‚Äúunregularized‚Äù $f_{MSE}$ is:</li>
</ul>

\[f_{MSE}(w) = \frac{1}{2n} \; \overset{n}{\underset{i=1}{\Sigma}} (y^{(i)} - \hat{y}^{(i)})^{2}\]

<ul>
  <li>
    <p>The $L_{2}$-regularized $f_{MSE}$ becomes:
\(f_{MSE}(w) = \frac{1}{2n} \; \overset{n}{\underset{i=1}{\Sigma}} (y^{(i)} - \hat{y}^{(i)})^{2} + \frac{\alpha}{2n}w^{T}w\)</p>

    <ul>
      <li>the points of $L_{2}$ is to ensure the weights $w$ will not grow too large</li>
      <li>the $\alpha$ term is a value used to determine how much you want to regularize the weights vs reduce the loss.</li>
    </ul>
  </li>
  <li>
    <p>To help with future comprehensions think of $L_{2}$ regularization as</p>
  </li>
</ul>

\[\text{MSE}_{L_{2}} = MSE + L_{2}\]

<p>This way its clear that the regularized term $\frac{\alpha}{2n}w^{T}w$ that is being added to MSE behaves as a penalty when weight values increase</p>

<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>

<p>The values we optimize when training a machine learning model - e.g., <strong>w</strong> and b for linear regression - are the parameters of the model.</p>

<p>There are also values related to the training process itself - e.g., learning rate $\epsilon$, batch size $\tilde{n}$ regularization strength $\alpha$ - which are the hyperparameters of training.</p>

<p>Both the parameters and hyperparameters can have a huge impact on model performance on test data.</p>

<p>When estimating the performance of a trained model, it is important to tune both kinds of parameters in a principled way:</p>

<ul>
  <li>Training/validation/testing sets</li>
  <li>Double cross-validation</li>
</ul>

<h4 id="trainingvalidationtesting-sets">Training/validation/testing sets:</h4>

<p>In an application domain with a large dataset (e.g., 100K examples), it is common to partition it into three subsets:</p>

<ul>
  <li>Training (typically 70-80%): optimization of parameters</li>
  <li>Validation (typically 5-10%): tuning of hyperparameters</li>
  <li>Testing (typically 5-10%): evaluation of the final model</li>
</ul>

<p>For comparison with other researchers‚Äô methods, this partition should be fixed.</p>

<p>Hyperparameter tuning works as follows:</p>

<ol>
  <li>For each hyperparameter configuration h:
    <ul>
      <li>Train the parameters on the training set using h.</li>
      <li>Evaluate the model on the validation set.</li>
      <li>If performance is better than what we got with the best h so far (h* ), then save h as h*</li>
    </ul>
  </li>
  <li>Train a model with h*, and evaluate its accuracy $A$ on the testing set. (You can train either on training data, or on training + validation data).</li>
</ol>

<h4 id="cross-validation">Cross-validation:</h4>

<p>When working with smaller datasets, cross-validation is commonly used so that we can use all data for training.</p>

<ul>
  <li>Suppose we already know the best hyperparameters h* .</li>
  <li>We partition the data into k folds of equal sizes.</li>
  <li>Over k iterations, we train on $(k-1)$ folds and test on the remaining fold.</li>
  <li>We then compute the average accuracy over the k testing folds.</li>
</ul>

<pre><code class="language-pseudocode"># D = dataset
# k = number of folds
# h = =hyperparameter configuration

def CrossValidation(D, k, h):
    # Partition D into k folds F_{1}, ..., F_{k}
    for i in range(len(k)):
        test_var = F_{i}
        train_var = D \ F_{i}
        # Train the model on train_var using h
        acc[i] = #Evaluate NN on test
    A = Avg[acc]
    return A
        
        
</code></pre>

<h4 id="trainingvalidationtesting-sets-continued">Training/validation/testing sets (Continued):</h4>

<p>Cross-validation does not measure the accuracy of any single machine.</p>

<p>Instead, cross-validation gives the expected accuracy of a classifier that is trained on $\frac{(k-1)}{k}$ of the data.</p>

<p>However, we can train another model $M$ using h* on the entire dataset, and then report $A$ as its accuracy.</p>

<p>Since $M$ is trained on more data than any of the crossvalidation models, its expected accuracy should be $\geq$ A.</p>

<h4 id="cross-validation-continued">Cross-Validation (continued):</h4>

<p>But how do we find the best hyperparameters h* for each fold?</p>

<p>The typical approach is to use double cross-validation, i.e.:</p>

<ul>
  <li>For each of the k ‚Äúouter‚Äù folds, run cross-validation in an ‚Äúinner‚Äù loop to determine the best hyperparameter configuration h* for the $k^{th}$ fold.</li>
</ul>

<h4 id="double-cross-validation">Double Cross-Validation:</h4>

<pre><code class="language-pseudocode"># D = dataset
# k = number of folds
# h = =hyperparameter configuration

def DoubleCrossValidation(D, k, H):
	# Partition D into k folds F_{1}, ..., F_{k}
	for i in range(len(k)):
		test_var = F_{i}
        train_var = D \ F_{i}
        A^{*} = # negative infinity
        For h in H:
        	A = CrossValidation(train_var, k, h)
        	if A &gt; A^{*}:
        		A^{*} = A
        		h* = h
      	Train the model on train_var using h* accs[i] = Evaluate 			the model on test_var
    A = Avg[accs]
    return A
        
</code></pre>

<h4 id="trainingvalidationtesting-sets-continued-again">Training/validation/testing sets (Continued Again):</h4>

<p>In contrast to (single) cross-validation, it‚Äôs not obvious how to train a model $M$ with accuracy $\geq$ $A$.</p>

<p>One strategy: return an ensemble model whose output is the average of the $k$ models‚Äô predictions‚Ä¶but this is rarely done.</p>

<h4 id="subject-independence">Subject Independence:</h4>

<p>In many machine learning settings, the data are not completely independent from each other - they are linked in some way.</p>

<p>Example:</p>

<ul>
  <li>Predict multiple grades for each student based on their Canvas clickstream features (# logins, # forum posts, etc.).</li>
</ul>

<p>We could partition the data into folds in different ways:</p>

<ul>
  <li>We could randomize across all the data.</li>
  <li>However, if grades are correlated within each student, then one (or more) training folds can leak information about the testing fold.</li>
  <li>Alternatively, we can stratify across students, i.e., no student appears in more than 1 fold.</li>
  <li>With this partition, the cross-validation accuracy estimates the model‚Äôs performance on a subject not used for training.</li>
</ul>

<p><img src="C:\Users\Bryan\OneDrive - Worcester Polytechnic Institute (wpi.edu)\Documents\Coding\github\Notes\notes\Deep-Learning\CS-541\pictures\table4.png" alt="" /></p>

:ET